{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification in Spark\n",
    "\n",
    "The intent of this blog is to demonstrate binary classification in pySpark. The various steps involved in developing a classification model in pySpark are as follows:\n",
    "\n",
    "1) Initialize a Spark session\n",
    "\n",
    "2) Download and read the the dataset\n",
    "\n",
    "3) Developing initial understanding about the data\n",
    "\n",
    "4) Handling missing values\n",
    "\n",
    "5) Scalerizing the features\n",
    "\n",
    "6) Train test split\n",
    "\n",
    "7) Imbalance handling\n",
    "\n",
    "8) Feature selection\n",
    "\n",
    "9) Performance evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing a Spark session\n",
    "from pyspark.sql import SparkSession\n",
    "import time\n",
    "spark = SparkSession.builder.master(\"local\").appName(\"flight\").config(\"spark.some.config.option\",\"some-value\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download and read the dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/hadoop\n",
      "diabetes.csv\t\t\t    logisticregression-flight.ipynb\n",
      "hw3.ipynb\t\t\t    logisticregression-Income.ipynb\n",
      "launchJupyter.sh\t\t    logisticregression-poker.ipynb\n",
      "logisticregression_diabeties.ipynb  spark_2.ipynb\n"
     ]
    }
   ],
   "source": [
    "!pwd\n",
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 13.2190270424 seconds ---\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Year',\n",
       " 'Month',\n",
       " 'DayofMonth',\n",
       " 'DayOfWeek',\n",
       " 'DepTime',\n",
       " 'CRSDepTime',\n",
       " 'ArrTime',\n",
       " 'CRSArrTime',\n",
       " 'UniqueCarrier',\n",
       " 'FlightNum',\n",
       " 'TailNum',\n",
       " 'ActualElapsedTime',\n",
       " 'CRSElapsedTime',\n",
       " 'AirTime',\n",
       " 'ArrDelay',\n",
       " 'DepDelay',\n",
       " 'Origin',\n",
       " 'Dest',\n",
       " 'Distance',\n",
       " 'TaxiIn',\n",
       " 'TaxiOut',\n",
       " 'Cancelled',\n",
       " 'CancellationCode',\n",
       " 'Diverted',\n",
       " 'CarrierDelay',\n",
       " 'WeatherDelay',\n",
       " 'NASDelay',\n",
       " 'SecurityDelay',\n",
       " 'LateAircraftDelay']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "raw_data = spark.read.csv('s3://516ml/airlinedelay.csv',\n",
    "                    header='true', inferSchema='true')\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "raw_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------+-------+--------+--------+------+--------+------------+------------+-------------+---------+\n",
      "|DayOfWeek|DepTime|AirTime|ArrDelay|DepDelay|Origin|Distance|CarrierDelay|WeatherDelay|SecurityDelay|Cancelled|\n",
      "+---------+-------+-------+--------+--------+------+--------+------------+------------+-------------+---------+\n",
      "|        4|   2003|    116|     -14|       8|   IAD|     810|        null|        null|         null|        0|\n",
      "|        4|    754|    113|       2|      19|   IAD|     810|        null|        null|         null|        0|\n",
      "|        4|    628|     76|      14|       8|   IND|     515|        null|        null|         null|        0|\n",
      "|        4|   1829|     77|      34|      34|   IND|     515|           2|           0|            0|        0|\n",
      "|        4|   1940|     87|      11|      25|   IND|     688|        null|        null|         null|        0|\n",
      "|        4|   1937|    230|      57|      67|   IND|    1591|          10|           0|            0|        0|\n",
      "|        4|    706|    106|       1|       6|   IND|     828|        null|        null|         null|        0|\n",
      "|        4|   1644|    107|      80|      94|   IND|     828|           8|           0|            0|        0|\n",
      "|        4|   1029|     37|      11|       9|   IND|     162|        null|        null|         null|        0|\n",
      "|        4|   1452|    213|      15|      27|   IND|    1489|           3|           0|            0|        0|\n",
      "|        4|    754|    205|     -15|       9|   IND|    1489|        null|        null|         null|        0|\n",
      "|        4|   1323|    110|      16|      28|   IND|     838|           0|           0|            0|        0|\n",
      "|        4|   1416|     49|      37|      51|   ISP|     220|          12|           0|            0|        0|\n",
      "|        4|   1657|     47|      19|      32|   ISP|     220|           7|           0|            0|        0|\n",
      "|        4|   1900|     49|       6|      20|   ISP|     220|        null|        null|         null|        0|\n",
      "|        4|   1039|     47|      -7|       9|   ISP|     220|        null|        null|         null|        0|\n",
      "|        4|   1520|     50|      14|      25|   ISP|     220|        null|        null|         null|        0|\n",
      "|        4|   1422|    143|      47|      87|   ISP|    1093|          40|           0|            0|        0|\n",
      "|        4|   1954|    155|       4|      29|   ISP|    1093|        null|        null|         null|        0|\n",
      "|        4|   2107|    134|      64|      82|   ISP|     972|           5|           0|            0|        0|\n",
      "+---------+-------+-------+--------+--------+------+--------+------------+------------+-------------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cols = [\"DayOfWeek\",\"DepTime\",\"AirTime\", \"ArrDelay\", \"DepDelay\", \"Origin\", \"Distance\", \"CarrierDelay\", \"WeatherDelay\", \"SecurityDelay\", \"Cancelled\"]\n",
    "\n",
    "raw_data.select(cols).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pyspark.sql.functions import when\n",
    "raw_data=raw_data.withColumn(\"ArrDelay\",when(raw_data.ArrDelay > 0, 1.0).otherwise(0.0))\n",
    "cols = [\"DayOfWeek\",\"DepTime\",\"AirTime\", \"ArrDelay\", \"DepDelay\", \"Distance\", \"CarrierDelay\", \"WeatherDelay\", \"SecurityDelay\", \"Cancelled\"]\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "for col in raw_data.columns:\n",
    "     raw_data= raw_data.withColumn(col,F.col(col).cast(\"float\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------+-------+--------+--------+--------+------------+------------+-------------+---------+\n",
      "|DayOfWeek|DepTime|AirTime|ArrDelay|DepDelay|Distance|CarrierDelay|WeatherDelay|SecurityDelay|Cancelled|\n",
      "+---------+-------+-------+--------+--------+--------+------------+------------+-------------+---------+\n",
      "|      4.0| 2003.0|  116.0|     0.0|     8.0|   810.0|        null|        null|         null|      0.0|\n",
      "|      4.0|  754.0|  113.0|     1.0|    19.0|   810.0|        null|        null|         null|      0.0|\n",
      "|      4.0|  628.0|   76.0|     1.0|     8.0|   515.0|        null|        null|         null|      0.0|\n",
      "|      4.0| 1829.0|   77.0|     1.0|    34.0|   515.0|         2.0|         0.0|          0.0|      0.0|\n",
      "|      4.0| 1940.0|   87.0|     1.0|    25.0|   688.0|        null|        null|         null|      0.0|\n",
      "+---------+-------+-------+--------+--------+--------+------------+------------+-------------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "raw_data.select(cols).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+---+----+---+----+---+----+---+----+-----+\n",
      "| s1|  c1| s2|  c2| s3|  c3| s4|  c4| s5|  c5|Class|\n",
      "+---+----+---+----+---+----+---+----+---+----+-----+\n",
      "|4.0| 7.0|3.0| 5.0|3.0| 3.0|1.0|13.0|4.0| 8.0|  0.0|\n",
      "|2.0| 8.0|4.0| 9.0|4.0| 6.0|4.0| 1.0|3.0| 7.0|  0.0|\n",
      "|3.0| 6.0|1.0| 3.0|2.0|11.0|3.0| 9.0|2.0| 3.0|  1.0|\n",
      "|2.0|10.0|2.0| 5.0|4.0|13.0|3.0| 9.0|1.0| 6.0|  0.0|\n",
      "|3.0| 2.0|1.0| 3.0|4.0| 7.0|3.0| 5.0|1.0|11.0|  0.0|\n",
      "|2.0| 1.0|3.0| 6.0|2.0|10.0|4.0|11.0|2.0| 6.0|  1.0|\n",
      "|2.0| 3.0|2.0| 4.0|3.0| 5.0|4.0|12.0|1.0| 6.0|  0.0|\n",
      "|4.0| 9.0|4.0| 7.0|3.0| 7.0|1.0| 6.0|2.0| 4.0|  1.0|\n",
      "|4.0| 7.0|2.0| 4.0|3.0| 7.0|2.0| 8.0|2.0|11.0|  1.0|\n",
      "|4.0| 8.0|3.0| 8.0|1.0|11.0|3.0| 5.0|2.0| 1.0|  1.0|\n",
      "|1.0| 7.0|3.0|10.0|3.0| 7.0|3.0| 4.0|2.0| 1.0|  1.0|\n",
      "|1.0| 3.0|4.0| 2.0|3.0|11.0|2.0| 2.0|3.0| 4.0|  1.0|\n",
      "|4.0| 1.0|2.0| 9.0|4.0| 6.0|3.0| 2.0|1.0| 3.0|  0.0|\n",
      "|2.0| 2.0|1.0| 2.0|4.0| 8.0|4.0|12.0|3.0| 4.0|  1.0|\n",
      "|4.0|11.0|1.0| 5.0|3.0| 6.0|2.0|13.0|4.0| 7.0|  0.0|\n",
      "|1.0| 9.0|2.0| 8.0|4.0|11.0|4.0| 9.0|1.0|11.0|  1.0|\n",
      "|1.0| 4.0|4.0|11.0|2.0|10.0|4.0| 1.0|4.0| 4.0|  1.0|\n",
      "|2.0|10.0|2.0|11.0|2.0| 3.0|2.0| 4.0|1.0| 2.0|  0.0|\n",
      "|4.0|13.0|2.0| 1.0|3.0| 5.0|1.0| 1.0|2.0| 7.0|  1.0|\n",
      "|3.0|11.0|3.0| 8.0|2.0| 5.0|2.0| 2.0|4.0| 5.0|  1.0|\n",
      "+---+----+---+----+---+----+---+----+---+----+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "raw_data.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we have replaced all \"0\" with NaN. Now, we can simply impute the NaN by calling an imputer :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+----------+---------+-------+----------+-------+----------+-------------+---------+-------+-----------------+--------------+-------+--------+--------+------+----+--------+------+-------+---------+----------------+--------+------------+------------+--------+-------------+-----------------+\n",
      "|  Year|Month|DayofMonth|DayOfWeek|DepTime|CRSDepTime|ArrTime|CRSArrTime|UniqueCarrier|FlightNum|TailNum|ActualElapsedTime|CRSElapsedTime|AirTime|ArrDelay|DepDelay|Origin|Dest|Distance|TaxiIn|TaxiOut|Cancelled|CancellationCode|Diverted|CarrierDelay|WeatherDelay|NASDelay|SecurityDelay|LateAircraftDelay|\n",
      "+------+-----+----------+---------+-------+----------+-------+----------+-------------+---------+-------+-----------------+--------------+-------+--------+--------+------+----+--------+------+-------+---------+----------------+--------+------------+------------+--------+-------------+-----------------+\n",
      "|2008.0|  1.0|       3.0|      4.0| 2003.0|    1955.0| 2211.0|    2225.0|         null|    335.0|   null|            128.0|         150.0|  116.0|     0.0|     8.0|  null|null|   810.0|   4.0|    8.0|      0.0|            null|     0.0|    18.87036|   3.5680323|    null|   0.09328398|             null|\n",
      "|2008.0|  1.0|       3.0|      4.0|  754.0|     735.0| 1002.0|    1000.0|         null|   3231.0|   null|            128.0|         145.0|  113.0|     1.0|    19.0|  null|null|   810.0|   5.0|   10.0|      0.0|            null|     0.0|    18.87036|   3.5680323|    null|   0.09328398|             null|\n",
      "|2008.0|  1.0|       3.0|      4.0|  628.0|     620.0|  804.0|     750.0|         null|    448.0|   null|             96.0|          90.0|   76.0|     1.0|     8.0|  null|null|   515.0|   3.0|   17.0|      0.0|            null|     0.0|    18.87036|   3.5680323|    null|   0.09328398|             null|\n",
      "|2008.0|  1.0|       3.0|      4.0| 1829.0|    1755.0| 1959.0|    1925.0|         null|   3920.0|   null|             90.0|          90.0|   77.0|     1.0|    34.0|  null|null|   515.0|   3.0|   10.0|      0.0|            null|     0.0|         2.0|         0.0|     0.0|          0.0|             32.0|\n",
      "|2008.0|  1.0|       3.0|      4.0| 1940.0|    1915.0| 2121.0|    2110.0|         null|    378.0|   null|            101.0|         115.0|   87.0|     1.0|    25.0|  null|null|   688.0|   4.0|   10.0|      0.0|            null|     0.0|    18.87036|   3.5680323|    null|   0.09328398|             null|\n",
      "+------+-----+----------+---------+-------+----------+-------+----------+-------------+---------+-------+-----------------+--------------+-------+--------+--------+------+----+--------+------+-------+---------+----------------+--------+------------+------------+--------+-------------+-----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import Imputer\n",
    "\n",
    "imputer=Imputer(inputCols=cols,outputCols=cols)\n",
    "model=imputer.fit(raw_data)\n",
    "raw_data=model.transform(raw_data)\n",
    "raw_data.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------------------------------------------------------------+\n",
      "|features                                                                                    |\n",
      "+--------------------------------------------------------------------------------------------+\n",
      "|[4.0,2003.0,116.0,8.0,810.0,18.870359420776367,3.5680322647094727,0.09328398108482361,0.0]  |\n",
      "|[4.0,754.0,113.0,19.0,810.0,18.870359420776367,3.5680322647094727,0.09328398108482361,0.0]  |\n",
      "|[4.0,628.0,76.0,8.0,515.0,18.870359420776367,3.5680322647094727,0.09328398108482361,0.0]    |\n",
      "|[4.0,1829.0,77.0,34.0,515.0,2.0,0.0,0.0,0.0]                                                |\n",
      "|[4.0,1940.0,87.0,25.0,688.0,18.870359420776367,3.5680322647094727,0.09328398108482361,0.0]  |\n",
      "|[4.0,1937.0,230.0,67.0,1591.0,10.0,0.0,0.0,0.0]                                             |\n",
      "|[4.0,706.0,106.0,6.0,828.0,18.870359420776367,3.5680322647094727,0.09328398108482361,0.0]   |\n",
      "|[4.0,1644.0,107.0,94.0,828.0,8.0,0.0,0.0,0.0]                                               |\n",
      "|[4.0,1029.0,37.0,9.0,162.0,18.870359420776367,3.5680322647094727,0.09328398108482361,0.0]   |\n",
      "|[4.0,1452.0,213.0,27.0,1489.0,3.0,0.0,0.0,0.0]                                              |\n",
      "|[4.0,754.0,205.0,9.0,1489.0,18.870359420776367,3.5680322647094727,0.09328398108482361,0.0]  |\n",
      "|[4.0,1323.0,110.0,28.0,838.0,0.0,0.0,0.0,0.0]                                               |\n",
      "|[4.0,1416.0,49.0,51.0,220.0,12.0,0.0,0.0,0.0]                                               |\n",
      "|[4.0,1657.0,47.0,32.0,220.0,7.0,0.0,0.0,0.0]                                                |\n",
      "|[4.0,1900.0,49.0,20.0,220.0,18.870359420776367,3.5680322647094727,0.09328398108482361,0.0]  |\n",
      "|[4.0,1039.0,47.0,9.0,220.0,18.870359420776367,3.5680322647094727,0.09328398108482361,0.0]   |\n",
      "|[4.0,1520.0,50.0,25.0,220.0,18.870359420776367,3.5680322647094727,0.09328398108482361,0.0]  |\n",
      "|[4.0,1422.0,143.0,87.0,1093.0,40.0,0.0,0.0,0.0]                                             |\n",
      "|[4.0,1954.0,155.0,29.0,1093.0,18.870359420776367,3.5680322647094727,0.09328398108482361,0.0]|\n",
      "|[4.0,2107.0,134.0,82.0,972.0,5.0,0.0,0.0,0.0]                                               |\n",
      "+--------------------------------------------------------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cols.remove(\"ArrDelay\")\n",
    "# Let us import the vector assembler\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "assembler = VectorAssembler(inputCols=cols,outputCol=\"features\")\n",
    "# Now let us use the transform method to transform our dataset\n",
    "raw_data=assembler.transform(raw_data)\n",
    "raw_data.select(\"features\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Standard Sclarizer \n",
    "\n",
    "So we have created a feature vector. Now let us use StandardScaler to scalerize the newly created \"feature\" column "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|            features|     Scaled_features|\n",
      "+--------------------+--------------------+\n",
      "|[4.0,2003.0,116.0...|[2.02326575050239...|\n",
      "|[4.0,754.0,113.0,...|[2.02326575050239...|\n",
      "|[4.0,628.0,76.0,8...|[2.02326575050239...|\n",
      "|[4.0,1829.0,77.0,...|[2.02326575050239...|\n",
      "|[4.0,1940.0,87.0,...|[2.02326575050239...|\n",
      "+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StandardScaler\n",
    "standardscaler=StandardScaler().setInputCol(\"features\").setOutputCol(\"Scaled_features\")\n",
    "raw_data=standardscaler.fit(raw_data).transform(raw_data)\n",
    "raw_data.select(\"features\",\"Scaled_features\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Train, test split\n",
    "\n",
    "Now that the preprocessing of the data is complete. Let us split the dataset in training and testing set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = raw_data.randomSplit([0.8, 0.2], seed=12345)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us check whether their is imbalance in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of ones are 751692\n",
      "Percentage of ones are 89.6679732699\n"
     ]
    }
   ],
   "source": [
    "dataset_size=float(train.select(\"ArrDelay\").count())\n",
    "numPositives=train.select(\"ArrDelay\").where('ArrDelay == 1').count()\n",
    "per_ones=(float(numPositives)/float(dataset_size))*100\n",
    "numNegatives=float(dataset_size-numPositives)\n",
    "print('The number of ones are {}'.format(numPositives))\n",
    "print('Percentage of ones are {}'.format(per_ones))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imbalancing handling\n",
    "\n",
    "Since the percentage of ones in the dataset is just 89.67 % surely there is imbalance in the dataset.  For this purpose we calculate the BalancingRatio as follows:\n",
    "\n",
    "BalancingRatio= numNegatives/dataset_size\n",
    "\n",
    "Then against every Outcome == 1, we put BalancingRatio in column \"classWeights\", and  against every Outcome == 0, we put 1-BalancingRatio in column  \"classWeights\" \n",
    "\n",
    "In this way, we assign higher weightage to the minority class (i.e. positive class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BalancingRatio = 0.103320267301\n"
     ]
    }
   ],
   "source": [
    "BalancingRatio= numNegatives/dataset_size\n",
    "print('BalancingRatio = {}'.format(BalancingRatio))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|       classWeights|\n",
      "+-------------------+\n",
      "|0.10332026730096171|\n",
      "|0.10332026730096171|\n",
      "|0.10332026730096171|\n",
      "| 0.8966797326990383|\n",
      "|0.10332026730096171|\n",
      "+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import when\n",
    "train=train.withColumn(\"classWeights\", when(train.ArrDelay == 1,BalancingRatio).otherwise(1-BalancingRatio))\n",
    "train.select(\"classWeights\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a classification model using Logistic Regression (LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 30.4018301964 seconds ---\n",
      "+--------+----------+\n",
      "|ArrDelay|prediction|\n",
      "+--------+----------+\n",
      "|     1.0|       0.0|\n",
      "|     1.0|       1.0|\n",
      "|     1.0|       1.0|\n",
      "|     1.0|       0.0|\n",
      "|     1.0|       1.0|\n",
      "|     1.0|       1.0|\n",
      "|     1.0|       0.0|\n",
      "|     1.0|       0.0|\n",
      "|     1.0|       1.0|\n",
      "|     1.0|       1.0|\n",
      "+--------+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "start_time = time.time()\n",
    "lr = LogisticRegression(labelCol=\"ArrDelay\", featuresCol=\"Scaled_features\",weightCol=\"classWeights\",maxIter=10)\n",
    "model = lr.fit(train)    \n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "predict_train=model.transform(train)\n",
    "predict_test=model.transform(test)\n",
    "\n",
    "predict_test.select(\"ArrDelay\",\"prediction\").show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating the model\n",
    "\n",
    "Now let us evaluate the model using BinaryClassificationEvaluator class in Spark ML. BinaryClassificationEvaluator by default uses areaUnderROC as the performance metric "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The BinaryClassificationEvaluator uses areaUnderROC as the default metric. As o fnow we will continue with the same\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "evaluator=BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\",labelCol=\"ArrDelay\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+----------+--------------------+\n",
      "|ArrDelay|       rawPrediction|prediction|         probability|\n",
      "+--------+--------------------+----------+--------------------+\n",
      "|     1.0|[1.38552941723670...|       0.0|[0.79987758089153...|\n",
      "|     1.0|[-2.6706861162332...|       1.0|[0.06472542143979...|\n",
      "|     1.0|[-15.444727776943...|       1.0|[1.96082965673554...|\n",
      "|     1.0|[1.94633222302961...|       0.0|[0.87504615703462...|\n",
      "|     1.0|[-0.5710423256649...|       1.0|[0.36099634844705...|\n",
      "+--------+--------------------+----------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predict_test.select(\"ArrDelay\",\"rawPrediction\",\"prediction\",\"probability\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The area under ROC for train set is 0.90625283217\n",
      "The area under ROC for test set is 0.90603355168\n"
     ]
    }
   ],
   "source": [
    "print(\"The area under ROC for train set is {}\".format(evaluator.evaluate(predict_train)))\n",
    "print(\"The area under ROC for test set is {}\".format(evaluator.evaluate(predict_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyper parameters\n",
    "\n",
    "To this point we have developed a classification model using logistic regression. However, the working of logistic regression depends upon the on a number of parameters. As of now we have worked with only the default parameters. Now, let s try to tune the hyperparameters and see whether it make any difference.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you are unsure which parameters to tune pls use \"print(lr.explainParams())\" to get the list of parameters available for tuning  \n",
    "print(lr.explainParams())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# List of tunable parameters in LR\n",
    "\n",
    "1) aggregationDepth: suggested depth for treeAggregate (>= 2). (default: 2)\n",
    "\n",
    "2) elasticNetParam: the ElasticNet mixing parameter, in range [0, 1]. For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty. (default: 0.0)\n",
    "\n",
    "3) family: The name of family which is a description of the label distribution to be used in the model. Supported options: auto, binomial, multinomial (default: auto)\n",
    "\n",
    "4) featuresCol: features column name. (default: features, current: Aspect)\n",
    "\n",
    "5) fitIntercept: whether to fit an intercept term. (default: True)\n",
    "\n",
    "6) labelCol: label column name. (default: label, current: Outcome)\n",
    "\n",
    "7) maxIter: max number of iterations (>= 0). (default: 100, current: 10)\n",
    "\n",
    "8) predictionCol: prediction column name. (default: prediction)\n",
    "\n",
    "9) probabilityCol: Column name for predicted class conditional probabilities. Note: Not all models output well-calibrated probability estimates! These probabilities should be treated as confidences, not precise probabilities. (default: probability)\n",
    "\n",
    "10) rawPredictionCol: raw prediction (a.k.a. confidence) column name. (default: rawPrediction)\n",
    "\n",
    "11) regParam: regularization parameter (>= 0). (default: 0.0)\n",
    "\n",
    "12) standardization: whether to standardize the training features before fitting the model. (default: True)\n",
    "\n",
    "13) threshold: Threshold in binary classification prediction, in range [0, 1].\n",
    "\n",
    "14) If threshold and thresholds are both set, they must match.e.g. if threshold is p, then thresholds must be equal to [1-p, p]. (default: 0.5)\n",
    "\n",
    "15) thresholds: Thresholds in multi-class classification to adjust the probability of predicting each class. Array must have length equal to the number of classes, with values > 0, excepting that at most one value may be 0. The class with largest value p/t is predicted, where p is the original probability of that class and t is the class's threshold. (undefined)\n",
    "\n",
    "16) tol: the convergence tolerance for iterative algorithms (>= 0). (default: 1e-06)\n",
    "\n",
    "17) weightCol: weight column name. If this is not set or empty, we treat all instance weights as 1.0. (current: classWeights)\n",
    "\n",
    "\n",
    "Now let us tune some of these parameters and observe their effect on the performance of the algorithm.\n",
    "\n",
    "For the purpose of hyperparameter tuning we will consider the following parameters:\n",
    "\n",
    "1) aggregationDepth [2, 5, 10]\n",
    "\n",
    "2) elasticNetParam [0.0, 0.5, 1.0]\n",
    "\n",
    "3) fitIntercept [True / False]\n",
    "\n",
    "4) maxIter [10, 100, 1000]\n",
    "\n",
    "5) regParam [0.01, 0.5, 2.0]\n",
    "\n",
    "frist off all let us define a parameter grid as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "\n",
    "paramGrid = ParamGridBuilder()\\\n",
    "    .addGrid(lr.aggregationDepth,[2,5,10])\\\n",
    "    .addGrid(lr.elasticNetParam,[0.0, 0.5, 1.0])\\\n",
    "    .addGrid(lr.fitIntercept,[False, True])\\\n",
    "    .addGrid(lr.maxIter,[10, 100, 1000])\\\n",
    "    .addGrid(lr.regParam,[0.01, 0.5, 2.0]) \\\n",
    "    .build()\n",
    "\n",
    "# https://spark.apache.org/docs/2.1.0/ml-tuning.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-fold cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create 5-fold CrossValidator\n",
    "cv = CrossValidator(estimator=lr, estimatorParamMaps=paramGrid, evaluator=evaluator, numFolds=5)\n",
    "\n",
    "# Run cross validations\n",
    "cvModel = cv.fit(train)\n",
    "# this will likely take a fair amount of time because of the amount of models that we're creating and testing\n",
    "predict_train=cvModel.transform(train)\n",
    "predict_test=cvModel.transform(test)\n",
    "print(\"The area under ROC for train set after CV  is {}\".format(evaluator.evaluate(predict_train)))\n",
    "print(\"The area under ROC for test set after CV  is {}\".format(evaluator.evaluate(predict_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1048575, 29)\n"
     ]
    }
   ],
   "source": [
    "print((raw_data.count(), len(raw_data.columns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
