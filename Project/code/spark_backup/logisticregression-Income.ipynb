{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification in Spark\n",
    "\n",
    "The intent of this blog is to demonstrate binary classification in pySpark. The various steps involved in developing a classification model in pySpark are as follows:\n",
    "\n",
    "1) Initialize a Spark session\n",
    "\n",
    "2) Download and read the the dataset\n",
    "\n",
    "3) Developing initial understanding about the data\n",
    "\n",
    "4) Handling missing values\n",
    "\n",
    "5) Scalerizing the features\n",
    "\n",
    "6) Train test split\n",
    "\n",
    "7) Imbalance handling\n",
    "\n",
    "8) Feature selection\n",
    "\n",
    "9) Performance evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing a Spark session\n",
    "from pyspark.sql import SparkSession\n",
    "import time\n",
    "spark = SparkSession.builder.master(\"local\").appName(\"income\").config(\"spark.some.config.option\",\"some-value\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download and read the dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 11.9728660583 seconds ---\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Age',\n",
       " 'Workclass',\n",
       " 'fnlgwt',\n",
       " 'Education',\n",
       " 'Education num',\n",
       " 'Marital Status',\n",
       " 'Occupation',\n",
       " 'Relationship',\n",
       " 'Race',\n",
       " 'Sex',\n",
       " 'Capital Gain',\n",
       " 'Capital Loss',\n",
       " 'Hours/Week',\n",
       " 'Native country',\n",
       " 'Income']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "raw_data = spark.read.csv('s3://516ml/adult-training.csv',\n",
    "                    header='true', inferSchema='true')\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "raw_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------------+--------+----------+-------------+-------------------+------------------+--------------+------+-------+------------+------------+----------+--------------+------+\n",
      "|Age|        Workclass|  fnlgwt| Education|Education num|     Marital Status|        Occupation|  Relationship|  Race|    Sex|Capital Gain|Capital Loss|Hours/Week|Native country|Income|\n",
      "+---+-----------------+--------+----------+-------------+-------------------+------------------+--------------+------+-------+------------+------------+----------+--------------+------+\n",
      "| 39|        State-gov| 77516.0| Bachelors|         13.0|      Never-married|      Adm-clerical| Not-in-family| White|   Male|      2174.0|         0.0|      40.0| United-States| <=50K|\n",
      "| 50| Self-emp-not-inc| 83311.0| Bachelors|         13.0| Married-civ-spouse|   Exec-managerial|       Husband| White|   Male|         0.0|         0.0|      13.0| United-States| <=50K|\n",
      "| 38|          Private|215646.0|   HS-grad|          9.0|           Divorced| Handlers-cleaners| Not-in-family| White|   Male|         0.0|         0.0|      40.0| United-States| <=50K|\n",
      "| 53|          Private|234721.0|      11th|          7.0| Married-civ-spouse| Handlers-cleaners|       Husband| Black|   Male|         0.0|         0.0|      40.0| United-States| <=50K|\n",
      "| 28|          Private|338409.0| Bachelors|         13.0| Married-civ-spouse|    Prof-specialty|          Wife| Black| Female|         0.0|         0.0|      40.0|          Cuba| <=50K|\n",
      "+---+-----------------+--------+----------+-------------+-------------------+------------------+--------------+------+-------+------------+------------+----------+--------------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "raw_data.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+------------------+-----------------+-------+------------------+------+\n",
      "|               Age|            fnlgwt|    Education num|    Sex|        Hours/Week|Income|\n",
      "+------------------+------------------+-----------------+-------+------------------+------+\n",
      "|             32561|             32561|            32561|  32561|             32561| 32561|\n",
      "| 38.58164675532078|189778.36651208502| 10.0806793403151|   null|40.437455852092995|  null|\n",
      "|13.640432553581356|105549.97769702227|2.572720332067397|   null|12.347428681731838|  null|\n",
      "|                17|           12285.0|              1.0| Female|               1.0| <=50K|\n",
      "|                90|         1484705.0|             16.0|   Male|              99.0|  >50K|\n",
      "+------------------+------------------+-----------------+-------+------------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "raw_data.describe().select(\"Age\",\"fnlgwt\",\"Education num\", \"Sex\", \"Hours/Week\", \"Income\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|Income|\n",
      "+------+\n",
      "| <=50K|\n",
      "| <=50K|\n",
      "| <=50K|\n",
      "| <=50K|\n",
      "| <=50K|\n",
      "| <=50K|\n",
      "| <=50K|\n",
      "|  >50K|\n",
      "|  >50K|\n",
      "|  >50K|\n",
      "|  >50K|\n",
      "|  >50K|\n",
      "| <=50K|\n",
      "| <=50K|\n",
      "|  >50K|\n",
      "| <=50K|\n",
      "| <=50K|\n",
      "| <=50K|\n",
      "| <=50K|\n",
      "|  >50K|\n",
      "+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from pyspark.sql.functions import when\n",
    "raw_data=raw_data.withColumn(\"Income\",when(raw_data.Income=='null',np.nan).otherwise(raw_data.Income))\n",
    "raw_data.select(\"Income\").show()\n",
    "raw_data=raw_data.withColumn(\"Income\",when(raw_data.Income==' <=50K',0.0).otherwise(1.0))\n",
    "\n",
    "raw_data=raw_data.withColumn(\"Sex\",when(raw_data.Sex=='null',np.nan).otherwise(raw_data.Sex))\n",
    "raw_data=raw_data.withColumn(\"Sex\",when(raw_data.Sex=='Female',1.0).otherwise(0.0))\n",
    "raw_data=raw_data.withColumn(\"Age\",raw_data.Age.cast('float'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|Income|\n",
      "+------+\n",
      "|   0.0|\n",
      "|   0.0|\n",
      "|   0.0|\n",
      "|   0.0|\n",
      "|   0.0|\n",
      "|   0.0|\n",
      "|   0.0|\n",
      "|   1.0|\n",
      "|   1.0|\n",
      "|   1.0|\n",
      "|   1.0|\n",
      "|   1.0|\n",
      "|   0.0|\n",
      "|   0.0|\n",
      "|   1.0|\n",
      "|   0.0|\n",
      "|   0.0|\n",
      "|   0.0|\n",
      "|   0.0|\n",
      "|   1.0|\n",
      "+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "raw_data.select(\"Income\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------+-------------+---+----------+------+\n",
      "| Age|  fnlgwt|Education num|Sex|Hours/Week|Income|\n",
      "+----+--------+-------------+---+----------+------+\n",
      "|39.0| 77516.0|         13.0|0.0|      40.0|   0.0|\n",
      "|50.0| 83311.0|         13.0|0.0|      13.0|   0.0|\n",
      "|38.0|215646.0|          9.0|0.0|      40.0|   0.0|\n",
      "|53.0|234721.0|          7.0|0.0|      40.0|   0.0|\n",
      "|28.0|338409.0|         13.0|0.0|      40.0|   0.0|\n",
      "+----+--------+-------------+---+----------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "raw_data.select(\"Age\",\"fnlgwt\",\"Education num\", \"Sex\", \"Hours/Week\", \"Income\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we have replaced all \"0\" with NaN. Now, we can simply impute the NaN by calling an imputer :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----------------+--------+----------+-------------+-------------------+------------------+--------------+------+---+------------+------------+----------+--------------+------+\n",
      "| Age|        Workclass|  fnlgwt| Education|Education num|     Marital Status|        Occupation|  Relationship|  Race|Sex|Capital Gain|Capital Loss|Hours/Week|Native country|Income|\n",
      "+----+-----------------+--------+----------+-------------+-------------------+------------------+--------------+------+---+------------+------------+----------+--------------+------+\n",
      "|39.0|        State-gov| 77516.0| Bachelors|         13.0|      Never-married|      Adm-clerical| Not-in-family| White|0.0|      2174.0|         0.0|      40.0| United-States|   0.0|\n",
      "|50.0| Self-emp-not-inc| 83311.0| Bachelors|         13.0| Married-civ-spouse|   Exec-managerial|       Husband| White|0.0|         0.0|         0.0|      13.0| United-States|   0.0|\n",
      "|38.0|          Private|215646.0|   HS-grad|          9.0|           Divorced| Handlers-cleaners| Not-in-family| White|0.0|         0.0|         0.0|      40.0| United-States|   0.0|\n",
      "|53.0|          Private|234721.0|      11th|          7.0| Married-civ-spouse| Handlers-cleaners|       Husband| Black|0.0|         0.0|         0.0|      40.0| United-States|   0.0|\n",
      "|28.0|          Private|338409.0| Bachelors|         13.0| Married-civ-spouse|    Prof-specialty|          Wife| Black|0.0|         0.0|         0.0|      40.0|          Cuba|   0.0|\n",
      "+----+-----------------+--------+----------+-------------+-------------------+------------------+--------------+------+---+------------+------------+----------+--------------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import Imputer\n",
    "imputer=Imputer(inputCols=[\"Age\",\"fnlgwt\",\"Education num\", \"Sex\", \"Hours/Week\", \"Income\"],outputCols=[\"Age\",\"fnlgwt\",\"Education num\", \"Sex\", \"Hours/Week\", \"Income\"])\n",
    "model=imputer.fit(raw_data)\n",
    "raw_data=model.transform(raw_data)\n",
    "raw_data.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------+\n",
      "|features                     |\n",
      "+-----------------------------+\n",
      "|[39.0,77516.0,13.0,0.0,40.0] |\n",
      "|[50.0,83311.0,13.0,0.0,13.0] |\n",
      "|[38.0,215646.0,9.0,0.0,40.0] |\n",
      "|[53.0,234721.0,7.0,0.0,40.0] |\n",
      "|[28.0,338409.0,13.0,0.0,40.0]|\n",
      "|[37.0,284582.0,14.0,0.0,40.0]|\n",
      "|[49.0,160187.0,5.0,0.0,16.0] |\n",
      "|[52.0,209642.0,9.0,0.0,45.0] |\n",
      "|[31.0,45781.0,14.0,0.0,50.0] |\n",
      "|[42.0,159449.0,13.0,0.0,40.0]|\n",
      "|[37.0,280464.0,10.0,0.0,80.0]|\n",
      "|[30.0,141297.0,13.0,0.0,40.0]|\n",
      "|[23.0,122272.0,13.0,0.0,30.0]|\n",
      "|[32.0,205019.0,12.0,0.0,50.0]|\n",
      "|[40.0,121772.0,11.0,0.0,40.0]|\n",
      "|[34.0,245487.0,4.0,0.0,45.0] |\n",
      "|[25.0,176756.0,9.0,0.0,35.0] |\n",
      "|[32.0,186824.0,9.0,0.0,40.0] |\n",
      "|[38.0,28887.0,7.0,0.0,50.0]  |\n",
      "|[43.0,292175.0,14.0,0.0,45.0]|\n",
      "+-----------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cols = [\"Age\",\"fnlgwt\",\"Education num\", \"Sex\", \"Hours/Week\", \"Income\"]\n",
    "cols.remove(\"Income\")\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "assembler = VectorAssembler(inputCols=cols,outputCol=\"features\")\n",
    "raw_data=assembler.transform(raw_data)\n",
    "raw_data.select(\"features\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Standard Sclarizer \n",
    "\n",
    "So we have created a feature vector. Now let us use StandardScaler to scalerize the newly created \"feature\" column "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|            features|     Scaled_features|\n",
      "+--------------------+--------------------+\n",
      "|[39.0,77516.0,13....|[2.85914686699289...|\n",
      "|[50.0,83311.0,13....|[3.66557290640114...|\n",
      "|[38.0,215646.0,9....|[2.78583540886487...|\n",
      "|[53.0,234721.0,7....|[3.88550728078521...|\n",
      "|[28.0,338409.0,13...|[2.05272082758464...|\n",
      "+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StandardScaler\n",
    "standardscaler=StandardScaler().setInputCol(\"features\").setOutputCol(\"Scaled_features\")\n",
    "raw_data=standardscaler.fit(raw_data).transform(raw_data)\n",
    "raw_data.select(\"features\",\"Scaled_features\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Train, test split\n",
    "\n",
    "Now that the preprocessing of the data is complete. Let us split the dataset in training and testing set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = raw_data.randomSplit([0.8, 0.2], seed=12345)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us check whether their is imbalance in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of ones are 6222\n",
      "Percentage of ones are 24.039873271\n"
     ]
    }
   ],
   "source": [
    "dataset_size=float(train.select(\"Income\").count())\n",
    "numPositives=train.select(\"Income\").where('Income == 1').count()\n",
    "per_ones=(float(numPositives)/float(dataset_size))*100\n",
    "numNegatives=float(dataset_size-numPositives)\n",
    "print('The number of ones are {}'.format(numPositives))\n",
    "print('Percentage of ones are {}'.format(per_ones))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imbalancing handling\n",
    "\n",
    "Since the percentage of ones in the dataset is just 24.04 % surely their is imbalance in the dataset. \n",
    "\n",
    "Therefore,logistic loss objective function should treat the positive class (Outcome == 1) with higher weight. For this purpose we calculate the BalancingRatio as follows:\n",
    "\n",
    "BalancingRatio= numNegatives/dataset_size\n",
    "\n",
    "Then against every Outcome == 1, we put BalancingRatio in column \"classWeights\", and  against every Outcome == 0, we put 1-BalancingRatio in column  \"classWeights\" \n",
    "\n",
    "In this way, we assign higher weightage to the minority class (i.e. positive class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BalancingRatio = 0.75960126729\n"
     ]
    }
   ],
   "source": [
    "BalancingRatio= numNegatives/dataset_size\n",
    "print('BalancingRatio = {}'.format(BalancingRatio))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|       classWeights|\n",
      "+-------------------+\n",
      "|0.24039873270999146|\n",
      "|0.24039873270999146|\n",
      "|0.24039873270999146|\n",
      "|0.24039873270999146|\n",
      "|0.24039873270999146|\n",
      "+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train=train.withColumn(\"classWeights\", when(train.Income == 1,BalancingRatio).otherwise(1-BalancingRatio))\n",
    "train.select(\"classWeights\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a classification model using Logistic Regression (LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 2.93401193619 seconds ---\n",
      "+------+----------+\n",
      "|Income|prediction|\n",
      "+------+----------+\n",
      "|   0.0|       0.0|\n",
      "|   0.0|       0.0|\n",
      "|   0.0|       0.0|\n",
      "|   0.0|       0.0|\n",
      "|   0.0|       0.0|\n",
      "|   0.0|       0.0|\n",
      "|   0.0|       0.0|\n",
      "|   0.0|       0.0|\n",
      "|   0.0|       0.0|\n",
      "|   0.0|       0.0|\n",
      "+------+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "# lr = LogisticRegression().setWeightCol(\"classWeights\").setLabelCol(\"Outcome\").setFeaturesCol(\"Aspect\")\n",
    "start_time = time.time()\n",
    "# lr = LogisticRegression(labelCol=\"Income\", featuresCol=\"Aspect\",weightCol=\"classWeights\",maxIter=10)\n",
    "lr = LogisticRegression(labelCol=\"Income\", featuresCol=\"Scaled_features\",weightCol=\"classWeights\",maxIter=10)\n",
    "model = lr.fit(train)    \n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "predict_train=model.transform(train)\n",
    "predict_test=model.transform(test)\n",
    "\n",
    "predict_test.select(\"Income\",\"prediction\").show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating the model\n",
    "\n",
    "Now let us evaluate the model using BinaryClassificationEvaluator class in Spark ML. BinaryClassificationEvaluator by default uses areaUnderROC as the performance metric "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The BinaryClassificationEvaluator uses areaUnderROC as the default metric. As o fnow we will continue with the same\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "evaluator=BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\",labelCol=\"Income\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------------+----------+--------------------+\n",
      "|Income|       rawPrediction|prediction|         probability|\n",
      "+------+--------------------+----------+--------------------+\n",
      "|   0.0|[3.25412979493759...|       0.0|[0.96282122779656...|\n",
      "|   0.0|[3.07498513336678...|       0.0|[0.95584903204928...|\n",
      "|   0.0|[3.75138778546630...|       0.0|[0.97705376443933...|\n",
      "|   0.0|[3.04977377641950...|       0.0|[0.95477275879431...|\n",
      "|   0.0|[3.83494964851657...|       0.0|[0.97885437073768...|\n",
      "+------+--------------------+----------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predict_test.select(\"Income\",\"rawPrediction\",\"prediction\",\"probability\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The area under ROC for train set is {}\".format(evaluator.evaluate(predict_train)))\n",
    "print(\"The area under ROC for test set is {}\".format(evaluator.evaluate(predict_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyper parameters\n",
    "\n",
    "To this point we have developed a classification model using logistic regression. However, the working of logistic regression depends upon the on a number of parameters. As of now we have worked with only the default parameters. Now, let s try to tune the hyperparameters and see whether it make any difference.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you are unsure which parameters to tune pls use \"print(lr.explainParams())\" to get the list of parameters available for tuning  \n",
    "print(lr.explainParams())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# List of tunable parameters in LR\n",
    "\n",
    "1) aggregationDepth: suggested depth for treeAggregate (>= 2). (default: 2)\n",
    "\n",
    "2) elasticNetParam: the ElasticNet mixing parameter, in range [0, 1]. For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty. (default: 0.0)\n",
    "\n",
    "3) family: The name of family which is a description of the label distribution to be used in the model. Supported options: auto, binomial, multinomial (default: auto)\n",
    "\n",
    "4) featuresCol: features column name. (default: features, current: Aspect)\n",
    "\n",
    "5) fitIntercept: whether to fit an intercept term. (default: True)\n",
    "\n",
    "6) labelCol: label column name. (default: label, current: Outcome)\n",
    "\n",
    "7) maxIter: max number of iterations (>= 0). (default: 100, current: 10)\n",
    "\n",
    "8) predictionCol: prediction column name. (default: prediction)\n",
    "\n",
    "9) probabilityCol: Column name for predicted class conditional probabilities. Note: Not all models output well-calibrated probability estimates! These probabilities should be treated as confidences, not precise probabilities. (default: probability)\n",
    "\n",
    "10) rawPredictionCol: raw prediction (a.k.a. confidence) column name. (default: rawPrediction)\n",
    "\n",
    "11) regParam: regularization parameter (>= 0). (default: 0.0)\n",
    "\n",
    "12) standardization: whether to standardize the training features before fitting the model. (default: True)\n",
    "\n",
    "13) threshold: Threshold in binary classification prediction, in range [0, 1].\n",
    "\n",
    "14) If threshold and thresholds are both set, they must match.e.g. if threshold is p, then thresholds must be equal to [1-p, p]. (default: 0.5)\n",
    "\n",
    "15) thresholds: Thresholds in multi-class classification to adjust the probability of predicting each class. Array must have length equal to the number of classes, with values > 0, excepting that at most one value may be 0. The class with largest value p/t is predicted, where p is the original probability of that class and t is the class's threshold. (undefined)\n",
    "\n",
    "16) tol: the convergence tolerance for iterative algorithms (>= 0). (default: 1e-06)\n",
    "\n",
    "17) weightCol: weight column name. If this is not set or empty, we treat all instance weights as 1.0. (current: classWeights)\n",
    "\n",
    "\n",
    "Now let us tune some of these parameters and observe their effect on the performance of the algorithm.\n",
    "\n",
    "For the purpose of hyperparameter tuning we will consider the following parameters:\n",
    "\n",
    "1) aggregationDepth [2, 5, 10]\n",
    "\n",
    "2) elasticNetParam [0.0, 0.5, 1.0]\n",
    "\n",
    "3) fitIntercept [True / False]\n",
    "\n",
    "4) maxIter [10, 100, 1000]\n",
    "\n",
    "5) regParam [0.01, 0.5, 2.0]\n",
    "\n",
    "frist off all let us define a parameter grid as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "\n",
    "paramGrid = ParamGridBuilder()\\\n",
    "    .addGrid(lr.aggregationDepth,[2,5,10])\\\n",
    "    .addGrid(lr.elasticNetParam,[0.0, 0.5, 1.0])\\\n",
    "    .addGrid(lr.fitIntercept,[False, True])\\\n",
    "    .addGrid(lr.maxIter,[10, 100, 1000])\\\n",
    "    .addGrid(lr.regParam,[0.01, 0.5, 2.0]) \\\n",
    "    .build()\n",
    "\n",
    "# https://spark.apache.org/docs/2.1.0/ml-tuning.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-fold cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-852e9cdac3c9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Run cross validations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mcvModel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;31m# this will likely take a fair amount of time because of the amount of models that we're creating and testing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mpredict_train\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcvModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/spark/python/pyspark/ml/base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    130\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    133\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m             raise ValueError(\"Params must be either a param map or a list/tuple of param maps, \"\n",
      "\u001b[0;32m/usr/lib/spark/python/pyspark/ml/tuning.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m             \u001b[0mtasks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_parallelFitTasks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meva\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollectSubModelsParam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 304\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetric\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubModel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimap_unordered\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    305\u001b[0m                 \u001b[0mmetrics\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmetric\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mnFolds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mcollectSubModelsParam\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib64/python2.7/multiprocessing/pool.pyc\u001b[0m in \u001b[0;36mnext\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    658\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_index\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_length\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    659\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 660\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    661\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    662\u001b[0m                     \u001b[0mitem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_items\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpopleft\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib64/python2.7/threading.pyc\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout, balancing)\u001b[0m\n\u001b[1;32m    338\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    339\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 340\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    341\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0m__debug__\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    342\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_note\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"%s.wait(): got it\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Create 5-fold CrossValidator\n",
    "cv = CrossValidator(estimator=lr, estimatorParamMaps=paramGrid, evaluator=evaluator, numFolds=5)\n",
    "\n",
    "# Run cross validations\n",
    "cvModel = cv.fit(train)\n",
    "# this will likely take a fair amount of time because of the amount of models that we're creating and testing\n",
    "predict_train=cvModel.transform(train)\n",
    "predict_test=cvModel.transform(test)\n",
    "print(\"The area under ROC for train set after CV  is {}\".format(evaluator.evaluate(predict_train)))\n",
    "print(\"The area under ROC for test set after CV  is {}\".format(evaluator.evaluate(predict_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32561, 17)\n"
     ]
    }
   ],
   "source": [
    "print((raw_data.count(), len(raw_data.columns)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
