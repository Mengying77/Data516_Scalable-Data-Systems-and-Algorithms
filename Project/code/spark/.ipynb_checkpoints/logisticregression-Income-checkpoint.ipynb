{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing a Spark session\n",
    "from pyspark.sql import SparkSession\n",
    "import time\n",
    "spark = SparkSession.builder.master(\"local\").appName(\"income\").config(\"spark.some.config.option\",\"some-value\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 16.2218940258 seconds ---\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "raw_data = spark.read.csv('s3://516ml/adult-training.csv',\n",
    "                    header='true', inferSchema='true')\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pyspark.sql.functions import when\n",
    "raw_data=raw_data.withColumn(\"Income\",when(raw_data.Income=='null',np.nan).otherwise(raw_data.Income))\n",
    "raw_data=raw_data.withColumn(\"Income\",when(raw_data.Income==' <=50K',0.0).otherwise(1.0))\n",
    "\n",
    "raw_data=raw_data.withColumn(\"Sex\",when(raw_data.Sex=='null',np.nan).otherwise(raw_data.Sex))\n",
    "raw_data=raw_data.withColumn(\"Sex\",when(raw_data.Sex=='Female',1.0).otherwise(0.0))\n",
    "raw_data=raw_data.withColumn(\"Age\",raw_data.Age.cast('float'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Imputer\n",
    "imputer=Imputer(inputCols=[\"Age\",\"fnlgwt\",\"Education num\", \"Sex\", \"Hours/Week\", \"Income\"],outputCols=[\"Age\",\"fnlgwt\",\"Education num\", \"Sex\", \"Hours/Week\", \"Income\"])\n",
    "model=imputer.fit(raw_data)\n",
    "raw_data=model.transform(raw_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = [\"Age\",\"fnlgwt\",\"Education num\", \"Sex\", \"Hours/Week\", \"Income\"]\n",
    "cols.remove(\"Income\")\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "assembler = VectorAssembler(inputCols=cols,outputCol=\"features\")\n",
    "raw_data=assembler.transform(raw_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Standard Sclarizer \n",
    "\n",
    "So we have created a feature vector. Now let us use StandardScaler to scalerize the newly created \"feature\" column "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StandardScaler\n",
    "standardscaler=StandardScaler().setInputCol(\"features\").setOutputCol(\"Scaled_features\")\n",
    "raw_data=standardscaler.fit(raw_data).transform(raw_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Train, test split\n",
    "\n",
    "Now that the preprocessing of the data is complete. Let us split the dataset in training and testing set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = raw_data.randomSplit([0.8, 0.2], seed=12345)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us check whether their is imbalance in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_size=float(train.select(\"Income\").count())\n",
    "numPositives=train.select(\"Income\").where('Income == 1').count()\n",
    "per_ones=(float(numPositives)/float(dataset_size))*100\n",
    "numNegatives=float(dataset_size-numPositives)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "BalancingRatio= numNegatives/dataset_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train=train.withColumn(\"classWeights\", when(train.Income == 1,BalancingRatio).otherwise(1-BalancingRatio))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 7.94853496552 seconds ---\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "start_time = time.time()\n",
    "lr = LogisticRegression(labelCol=\"Income\", featuresCol=\"Scaled_features\",weightCol=\"classWeights\",maxIter=10)\n",
    "model = lr.fit(train)    \n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "predict_train=model.transform(train)\n",
    "predict_test=model.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The BinaryClassificationEvaluator uses areaUnderROC as the default metric. As o fnow we will continue with the same\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "evaluator=BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\",labelCol=\"Income\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------------+----------+--------------------+\n",
      "|Income|       rawPrediction|prediction|         probability|\n",
      "+------+--------------------+----------+--------------------+\n",
      "|   0.0|[3.25412979493759...|       0.0|[0.96282122779656...|\n",
      "|   0.0|[3.07498513336678...|       0.0|[0.95584903204928...|\n",
      "|   0.0|[3.75138778546630...|       0.0|[0.97705376443933...|\n",
      "|   0.0|[3.04977377641950...|       0.0|[0.95477275879431...|\n",
      "|   0.0|[3.83494964851657...|       0.0|[0.97885437073768...|\n",
      "+------+--------------------+----------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predict_test.select(\"Income\",\"rawPrediction\",\"prediction\",\"probability\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The area under ROC for train set is 0.802233893908\n",
      "The area under ROC for test set is 0.790478006968\n"
     ]
    }
   ],
   "source": [
    "print(\"The area under ROC for train set is {}\".format(evaluator.evaluate(predict_train)))\n",
    "print(\"The area under ROC for test set is {}\".format(evaluator.evaluate(predict_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "\n",
    "paramGrid = ParamGridBuilder()\\\n",
    "    .addGrid(lr.aggregationDepth,[2,5,10])\\\n",
    "    .addGrid(lr.elasticNetParam,[0.0, 0.5, 1.0])\\\n",
    "    .addGrid(lr.fitIntercept,[False, True])\\\n",
    "    .addGrid(lr.maxIter,[10, 100, 1000])\\\n",
    "    .addGrid(lr.regParam,[0.01, 0.5, 2.0]) \\\n",
    "    .build()\n",
    "\n",
    "# https://spark.apache.org/docs/2.1.0/ml-tuning.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-fold cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 1388.87217283 seconds ---\n"
     ]
    }
   ],
   "source": [
    "# Create 5-fold CrossValidator\n",
    "cv = CrossValidator(estimator=lr, estimatorParamMaps=paramGrid, evaluator=evaluator, numFolds=5)\n",
    "start_time = time.time()\n",
    "# Run cross validations\n",
    "cvModel = cv.fit(train)\n",
    "# this will likely take a fair amount of time because of the amount of models that we're creating and testing\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "\n",
    "# predict_train=cvModel.transform(train)\n",
    "# predict_test=cvModel.transform(test)\n",
    "\n",
    "# print(\"The area under ROC for train set after CV  is {}\".format(evaluator.evaluate(predict_train)))\n",
    "# print(\"The area under ROC for test set after CV  is {}\".format(evaluator.evaluate(predict_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32561, 17)\n"
     ]
    }
   ],
   "source": [
    "print((raw_data.count(), len(raw_data.columns)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
